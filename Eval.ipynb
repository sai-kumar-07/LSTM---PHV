{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8063f724-2d5d-4f0e-8040-43b02176ca3f",
   "metadata": {},
   "source": [
    "### Task1\n",
    "#### Mapping the random_int_1.txt names to the actual sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eac42b5-93e5-4f12-ae44-d89fa2f6ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pairs:\n",
      "                   dengue_id human_id\n",
      "0  YP_001531165.2_capsid_DV3    HOXB7\n",
      "1  YP_001531165.2_capsid_DV3   SPACA3\n",
      "2  YP_001531165.2_capsid_DV3   RNF168\n",
      "3  YP_001531165.2_capsid_DV3     SCO2\n",
      "4  YP_001531165.2_capsid_DV3  HMGCLL1\n",
      "\n",
      "Parsing dengue sequences...\n",
      "Found 60 dengue sequences\n",
      "\n",
      "Parsing human sequences...\n",
      "Found 17806 human sequences\n",
      "\n",
      "Successfully saved 1980 matched pairs\n",
      "Sample output:\n",
      "                   dengue_id human_id  \\\n",
      "0  YP_001531165.2_capsid_DV3    HOXB7   \n",
      "1  YP_001531165.2_capsid_DV3   SPACA3   \n",
      "2  YP_001531165.2_capsid_DV3   RNF168   \n",
      "3  YP_001531165.2_capsid_DV3     SCO2   \n",
      "4  YP_001531165.2_capsid_DV3  HMGCLL1   \n",
      "\n",
      "                                     dengue_sequence  \\\n",
      "0  MNNQRKKTGKPSINMLKRVRNRVSTGSQLAKRFSKGLLNGQGPMKL...   \n",
      "1  MNNQRKKTGKPSINMLKRVRNRVSTGSQLAKRFSKGLLNGQGPMKL...   \n",
      "2  MNNQRKKTGKPSINMLKRVRNRVSTGSQLAKRFSKGLLNGQGPMKL...   \n",
      "3  MNNQRKKTGKPSINMLKRVRNRVSTGSQLAKRFSKGLLNGQGPMKL...   \n",
      "4  MNNQRKKTGKPSINMLKRVRNRVSTGSQLAKRFSKGLLNGQGPMKL...   \n",
      "\n",
      "                                      human_sequence  \n",
      "0  MSSLYYANTLFSKYPASSSVFATGAFPEQTSCAFASNPQRPGYGAG...  \n",
      "1  MVSALRGAPLIRVCLAYFTSGFNAAALDYEADGSTNNGIFQINSRR...  \n",
      "2  MALPKDAIPSLSECQCGICMEILVEPVTLPCNHTLCKPCFQSTVEK...  \n",
      "3  MLLLTRSPTAWHRLSQLKPRVLPGTLGGQALHLRSWLLSRQGPAET...  \n",
      "4  MGNVPSAVKHCLSYQQLLREHLWIGDSVAGALDPAQETSQLSGLPE...  \n",
      "\n",
      "Processing Summary:\n",
      "Total input pairs: 2220\n",
      "Matched pairs: 1980\n",
      "Pairs missing dengue: 240\n",
      "Pairs missing human: 0\n",
      "Unmatched rate: 10.8%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_fasta(filename, strip_char=None):\n",
    "    \"\"\"Parse FASTA file into a dictionary, optionally stripping characters from sequences\"\"\"\n",
    "    sequences = {}\n",
    "    current_id = None\n",
    "    current_seq = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if current_id is not None:\n",
    "                    # Save previous sequence\n",
    "                    full_seq = ''.join(current_seq)\n",
    "                    if strip_char:\n",
    "                        full_seq = full_seq.rstrip(strip_char)\n",
    "                    sequences[current_id] = full_seq\n",
    "                # Get new ID (remove '>' and any leading/trailing whitespace)\n",
    "                current_id = line[1:].split()[0].strip()\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "        \n",
    "        # Add the last sequence\n",
    "        if current_id is not None:\n",
    "            full_seq = ''.join(current_seq)\n",
    "            if strip_char:\n",
    "                full_seq = full_seq.rstrip(strip_char)\n",
    "            sequences[current_id] = full_seq\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Load the pairs file\n",
    "try:\n",
    "    pairs_df = pd.read_csv('random_int_1.txt', sep='\\t', header=None, \n",
    "                          names=['dengue_id', 'human_id'], dtype='string')\n",
    "    print(\"Successfully loaded pairs:\")\n",
    "    print(pairs_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading pairs file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Parse sequence files\n",
    "print(\"\\nParsing dengue sequences...\")\n",
    "dengue_sequences = parse_fasta('dengue_ni_sequences.txt')\n",
    "print(f\"Found {len(dengue_sequences)} dengue sequences\")\n",
    "\n",
    "print(\"\\nParsing human sequences...\")\n",
    "human_sequences = parse_fasta('identical_sequences.txt', strip_char='*')\n",
    "print(f\"Found {len(human_sequences)} human sequences\")\n",
    "\n",
    "# Match pairs with sequences\n",
    "matched_data = []\n",
    "missing_dengue = 0\n",
    "missing_human = 0\n",
    "\n",
    "for idx, row in pairs_df.iterrows():\n",
    "    dengue_id = row['dengue_id']\n",
    "    human_id = row['human_id']\n",
    "    \n",
    "    # Check both sequences exist\n",
    "    has_dengue = dengue_id in dengue_sequences\n",
    "    has_human = human_id in human_sequences\n",
    "    \n",
    "    if not has_dengue:\n",
    "        missing_dengue += 1\n",
    "    if not has_human:\n",
    "        missing_human += 1\n",
    "    \n",
    "    if has_dengue and has_human:\n",
    "        matched_data.append({\n",
    "            'dengue_id': dengue_id,\n",
    "            'human_id': human_id,\n",
    "            'dengue_sequence': dengue_sequences[dengue_id],\n",
    "            'human_sequence': human_sequences[human_id]\n",
    "        })\n",
    "\n",
    "# Create DataFrame and save\n",
    "if matched_data:\n",
    "    output_df = pd.DataFrame(matched_data)\n",
    "    output_df.to_csv('task1_sequences.csv', index=False)\n",
    "    print(f\"\\nSuccessfully saved {len(output_df)} matched pairs\")\n",
    "    print(\"Sample output:\")\n",
    "    print(output_df.head())\n",
    "else:\n",
    "    print(\"\\nNo matching pairs found!\")\n",
    "\n",
    "# Print summary stats\n",
    "print(\"\\nProcessing Summary:\")\n",
    "print(f\"Total input pairs: {len(pairs_df)}\")\n",
    "print(f\"Matched pairs: {len(matched_data)}\")\n",
    "print(f\"Pairs missing dengue: {missing_dengue}\")\n",
    "print(f\"Pairs missing human: {missing_human}\")\n",
    "print(f\"Unmatched rate: {(missing_dengue + missing_human)/len(pairs_df)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77215a3f-a2c1-4d6b-8c20-e33dfcf56109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   dengue_id human_id  \\\n",
      "0  YP_001531165.2_capsid_DV3    HOXB7   \n",
      "1  YP_001531165.2_capsid_DV3   SPACA3   \n",
      "2  YP_001531165.2_capsid_DV3   RNF168   \n",
      "3  YP_001531165.2_capsid_DV3     SCO2   \n",
      "4  YP_001531165.2_capsid_DV3  HMGCLL1   \n",
      "\n",
      "                                     dengue_sequence  \\\n",
      "0  MNNQRKKTGKPSINMLKRVRNRVSTGSQLAKRFSKGLLNGQGPMKL...   \n",
      "1  MNNQRKKTGKPSINMLKRVRNRVSTGSQLAKRFSKGLLNGQGPMKL...   \n",
      "2  MNNQRKKTGKPSINMLKRVRNRVSTGSQLAKRFSKGLLNGQGPMKL...   \n",
      "3  MNNQRKKTGKPSINMLKRVRNRVSTGSQLAKRFSKGLLNGQGPMKL...   \n",
      "4  MNNQRKKTGKPSINMLKRVRNRVSTGSQLAKRFSKGLLNGQGPMKL...   \n",
      "\n",
      "                                      human_sequence  \n",
      "0  MSSLYYANTLFSKYPASSSVFATGAFPEQTSCAFASNPQRPGYGAG...  \n",
      "1  MVSALRGAPLIRVCLAYFTSGFNAAALDYEADGSTNNGIFQINSRR...  \n",
      "2  MALPKDAIPSLSECQCGICMEILVEPVTLPCNHTLCKPCFQSTVEK...  \n",
      "3  MLLLTRSPTAWHRLSQLKPRVLPGTLGGQALHLRSWLLSRQGPAET...  \n",
      "4  MGNVPSAVKHCLSYQQLLREHLWIGDSVAGALDPAQETSQLSGLPE...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('task1_sequences.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e676c8-4dfc-4633-ab3d-9752b01b0b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ea7a296-32b7-4152-b4f9-070f43f5631f",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "#### Now we will take the file from task1 and get the top 100 kmers and map them to the corresponding kmer embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83412171-88dc-4d45-bb58-2f0952458881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading k-mer embeddings...\n",
      "Loaded 157689 k-mers with 128D embeddings\n",
      "Loading models...\n",
      "\n",
      "Model Architecture Verification:\n",
      "Human model input shape: (None, 100, 128)\n",
      "Human model output shape: (None, 100, 128)\n",
      "Dengue model input shape: (None, 100, 128)\n",
      "Dengue model output shape: (None, 100, 128)\n",
      "\n",
      "Processing human sequences...\n",
      "Saved 1980 human embeddings\n",
      "\n",
      "Processing dengue sequences...\n",
      "Saved 1980 dengue embeddings\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "KMER_LENGTH = 4\n",
    "TOP_KMERS = 100\n",
    "EMBEDDING_DIM = 128  # Dimension from k-mer embedding file\n",
    "LATENT_DIM = 128     # Final embedding dimension\n",
    "\n",
    "def load_kmer_embeddings(filename):\n",
    "    \"\"\"Load k-mer embeddings with validation\"\"\"\n",
    "    kmer_emb = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != EMBEDDING_DIM + 1:  # 1 k-mer + 128 values\n",
    "                continue\n",
    "            kmer = parts[0]\n",
    "            emb = list(map(float, parts[1:]))\n",
    "            kmer_emb[kmer] = emb\n",
    "    print(f\"Loaded {len(kmer_emb)} k-mers with {EMBEDDING_DIM}D embeddings\")\n",
    "    return kmer_emb\n",
    "\n",
    "def generate_kmers(sequence, k):\n",
    "    \"\"\"Generate k-mers with length validation\"\"\"\n",
    "    return [sequence[i:i+k] for i in range(len(sequence)-k+1) if len(sequence[i:i+k]) == k]\n",
    "\n",
    "def get_top_kmer_features(sequence, kmer_emb, k=4, top_n=100):\n",
    "    \"\"\"Get top k-mers with proper frequency handling\"\"\"\n",
    "    kmers = generate_kmers(sequence, k)\n",
    "    freq = defaultdict(int)\n",
    "    for kmer in kmers:\n",
    "        freq[kmer] += 1\n",
    "    \n",
    "    # Sort by frequency then alphabetically\n",
    "    sorted_kmers = sorted(freq.items(), \n",
    "                         key=lambda x: (-x[1], x[0]))[:top_n*2]\n",
    "    \n",
    "    # Select unique k-mers with embeddings\n",
    "    selected = []\n",
    "    for kmer, _ in sorted_kmers:\n",
    "        if kmer in kmer_emb and kmer not in selected:\n",
    "            selected.append(kmer)\n",
    "            if len(selected) == top_n:\n",
    "                break\n",
    "    \n",
    "    # Fill missing with zero vectors\n",
    "    embeddings = []\n",
    "    for kmer in selected[:top_n]:\n",
    "        embeddings.append(kmer_emb.get(kmer, [0.0]*EMBEDDING_DIM))\n",
    "    \n",
    "    # Pad if needed\n",
    "    while len(embeddings) < top_n:\n",
    "        embeddings.append([0.0]*EMBEDDING_DIM)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "def process_sequences(df, model, kmer_emb, id_col, sequence_col):\n",
    "    \"\"\"Process sequences and generate embeddings\"\"\"\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        seq_id = row[id_col]\n",
    "        sequence = row[sequence_col].replace('*', '')\n",
    "        \n",
    "        # Get k-mer features\n",
    "        kmer_features = get_top_kmer_features(sequence, kmer_emb)\n",
    "        \n",
    "        # Reshape for model input: (batch_size, timesteps, features)\n",
    "        input_data = kmer_features.reshape(1, TOP_KMERS, EMBEDDING_DIM)\n",
    "        \n",
    "        # Get latent embedding\n",
    "        embedding = model.predict(input_data, verbose=0)\n",
    "        \n",
    "        # Extract final 128D embedding (last timestep)\n",
    "        latent_embedding = embedding[0, -1, :].tolist()\n",
    "        \n",
    "        # Create row with separate columns for each embedding dimension\n",
    "        emb_dict = {f\"emb_{i+1}\": val for i, val in enumerate(latent_embedding)}\n",
    "        results.append({id_col: seq_id, **emb_dict})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load resources\n",
    "print(\"Loading k-mer embeddings...\")\n",
    "kmer_embeddings = load_kmer_embeddings('kmers_embedding_human_dengue.txt')\n",
    "\n",
    "print(\"Loading models...\")\n",
    "human_ae = tf.keras.models.load_model('drop_Human_bilstm_autoencoder_model.keras')\n",
    "dengue_ae = tf.keras.models.load_model('drop_Dengue_bilstm_autoencoder_model.keras')\n",
    "\n",
    "# Verify model output shapes\n",
    "print(\"\\nModel Architecture Verification:\")\n",
    "print(\"Human model input shape:\", human_ae.input_shape)\n",
    "print(\"Human model output shape:\", human_ae.output_shape)\n",
    "print(\"Dengue model input shape:\", dengue_ae.input_shape)\n",
    "print(\"Dengue model output shape:\", dengue_ae.output_shape)\n",
    "\n",
    "# Load sequences\n",
    "task1_df = pd.read_csv('task1_sequences.csv')\n",
    "\n",
    "# Process human sequences\n",
    "print(\"\\nProcessing human sequences...\")\n",
    "human_emb_df = process_sequences(task1_df, human_ae, kmer_embeddings, \n",
    "                                'human_id', 'human_sequence')\n",
    "human_emb_df.to_csv('human_embeddings_128d.csv', index=False)\n",
    "print(f\"Saved {len(human_emb_df)} human embeddings\")\n",
    "\n",
    "# Process dengue sequences\n",
    "print(\"\\nProcessing dengue sequences...\")\n",
    "dengue_emb_df = process_sequences(task1_df, dengue_ae, kmer_embeddings,\n",
    "                                 'dengue_id', 'dengue_sequence')\n",
    "dengue_emb_df.to_csv('dengue_embeddings_128d.csv', index=False)\n",
    "print(f\"Saved {len(dengue_emb_df)} dengue embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "312049f6-f173-420d-8b1d-323caaac6b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  human_id     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  \\\n",
      "0    HOXB7  0.000000  0.111181  0.000888  0.000000  0.649469  0.150634   \n",
      "1   SPACA3  0.000000  0.148559  0.000000  0.000000  0.132296  0.000000   \n",
      "2   RNF168  0.098054  0.000000  0.000000  0.730736  0.286459  0.000000   \n",
      "3     SCO2  0.495526  0.000000  0.000000  0.149431  0.699470  0.000000   \n",
      "4  HMGCLL1  0.000000  0.000000  0.149232  0.000000  0.000000  0.244725   \n",
      "\n",
      "      emb_7     emb_8  emb_9  ...   emb_119  emb_120   emb_121   emb_122  \\\n",
      "0  0.407491  0.442823    0.0  ...  0.056331      0.0  0.198479  0.000000   \n",
      "1  0.000000  0.771057    0.0  ...  0.000000      0.0  0.000000  0.000000   \n",
      "2  0.080425  0.000000    0.0  ...  0.000000      0.0  0.000000  0.007437   \n",
      "3  0.099769  0.000000    0.0  ...  0.000000      0.0  0.000000  0.000000   \n",
      "4  0.020758  0.000000    0.0  ...  0.000000      0.0  0.000000  0.239860   \n",
      "\n",
      "    emb_123   emb_124   emb_125  emb_126   emb_127  emb_128  \n",
      "0  0.000000  0.000000  0.000000      0.0  0.000000  0.00000  \n",
      "1  0.000000  0.000000  0.472694      0.0  0.000000  0.00000  \n",
      "2  0.092766  0.000000  0.193645      0.0  0.300548  0.30957  \n",
      "3  0.000000  0.270799  0.000000      0.0  0.497201  0.00000  \n",
      "4  0.000000  0.000000  0.881239      0.0  0.000000  0.00000  \n",
      "\n",
      "[5 rows x 129 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('human_embeddings_128d.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44a492f8-6e0a-4711-8117-14d6a3065e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   dengue_id     emb_1     emb_2  emb_3  emb_4  emb_5  \\\n",
      "0  YP_001531165.2_capsid_DV3  0.065721  0.105939    0.0    0.0    0.0   \n",
      "1  YP_001531165.2_capsid_DV3  0.065721  0.105939    0.0    0.0    0.0   \n",
      "2  YP_001531165.2_capsid_DV3  0.065721  0.105939    0.0    0.0    0.0   \n",
      "3  YP_001531165.2_capsid_DV3  0.065721  0.105939    0.0    0.0    0.0   \n",
      "4  YP_001531165.2_capsid_DV3  0.065721  0.105939    0.0    0.0    0.0   \n",
      "\n",
      "      emb_6  emb_7    emb_8  emb_9  ...   emb_119  emb_120  emb_121  emb_122  \\\n",
      "0  0.259708    0.0  0.12811    0.0  ...  0.045609      0.0      0.0      0.0   \n",
      "1  0.259708    0.0  0.12811    0.0  ...  0.045609      0.0      0.0      0.0   \n",
      "2  0.259708    0.0  0.12811    0.0  ...  0.045609      0.0      0.0      0.0   \n",
      "3  0.259708    0.0  0.12811    0.0  ...  0.045609      0.0      0.0      0.0   \n",
      "4  0.259708    0.0  0.12811    0.0  ...  0.045609      0.0      0.0      0.0   \n",
      "\n",
      "   emb_123  emb_124   emb_125  emb_126   emb_127  emb_128  \n",
      "0      0.0      0.0  0.136287      0.0  0.209839      0.0  \n",
      "1      0.0      0.0  0.136287      0.0  0.209839      0.0  \n",
      "2      0.0      0.0  0.136287      0.0  0.209839      0.0  \n",
      "3      0.0      0.0  0.136287      0.0  0.209839      0.0  \n",
      "4      0.0      0.0  0.136287      0.0  0.209839      0.0  \n",
      "\n",
      "[5 rows x 129 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dengue_embeddings_128d.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e0f5b-5211-4ef3-b15e-7318fa6b0b0e",
   "metadata": {},
   "source": [
    "### Task3\n",
    "#### Get the label for the given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e378bda-e683-47b1-83ad-38837f85211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prediction model...\n",
      "Model expects input dimension: 256\n",
      "\n",
      "Loading human embeddings...\n",
      "Loaded 1817 embeddings from human_embeddings_128d.csv\n",
      "Loading dengue embeddings...\n",
      "Loaded 53 embeddings from dengue_embeddings_128d.csv\n",
      "\n",
      "Processing 1980 pairs...\n",
      "\n",
      "Saved 1980 predictions to final_predictions.csv\n",
      "Sample predictions:\n",
      "                   dengue_id human_id  predicted_label    confidence\n",
      "0  YP_001531165.2_capsid_DV3    HOXB7                0  0.000000e+00\n",
      "1  YP_001531165.2_capsid_DV3   SPACA3                0  1.580929e-36\n",
      "2  YP_001531165.2_capsid_DV3   RNF168                0  4.733572e-22\n",
      "3  YP_001531165.2_capsid_DV3     SCO2                0  0.000000e+00\n",
      "4  YP_001531165.2_capsid_DV3  HMGCLL1                0  4.948700e-38\n",
      "\n",
      "Processing Summary:\n",
      "Total pairs: 1980\n",
      "Successful predictions: 1980\n",
      "Missing embeddings: 0\n",
      "Prediction rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configuration\n",
    "HUMAN_EMBED_FILE = 'human_embeddings_128d.csv'\n",
    "DENGUE_EMBED_FILE = 'dengue_embeddings_128d.csv'\n",
    "PAIRS_FILE = 'task1_sequences.csv'\n",
    "MODEL_PATH = 'fc_final_model.keras'\n",
    "OUTPUT_FILE = 'final_predictions.csv'\n",
    "\n",
    "def load_embeddings(file_path, id_col='human_id'):\n",
    "    \"\"\"Load embeddings from wide-format CSV with emb_1 to emb_128 columns\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract embedding columns\n",
    "    emb_cols = [col for col in df.columns if col.startswith('emb_')]\n",
    "    \n",
    "    # Validate embedding dimensions\n",
    "    if len(emb_cols) != 128:\n",
    "        raise ValueError(f\"Invalid embedding dimensions in {file_path}: \"\n",
    "                         f\"Expected 128 columns, found {len(emb_cols)}\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    embeddings = {}\n",
    "    for _, row in df.iterrows():\n",
    "        emb_array = np.array([row[col] for col in emb_cols], dtype=np.float32)\n",
    "        if emb_array.shape != (128,):\n",
    "            raise ValueError(f\"Invalid embedding shape for {row[id_col]}: \"\n",
    "                             f\"Expected (128,), got {emb_array.shape}\")\n",
    "        embeddings[row[id_col]] = emb_array\n",
    "    \n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {file_path}\")\n",
    "    return embeddings\n",
    "\n",
    "# Load pretrained model\n",
    "print(\"Loading prediction model...\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "expected_input_dim = model.input_shape[1]\n",
    "print(f\"Model expects input dimension: {expected_input_dim}\")\n",
    "\n",
    "# Load embeddings\n",
    "print(\"\\nLoading human embeddings...\")\n",
    "human_embeddings = load_embeddings(HUMAN_EMBED_FILE, 'human_id')\n",
    "print(\"Loading dengue embeddings...\")\n",
    "dengue_embeddings = load_embeddings(DENGUE_EMBED_FILE, 'dengue_id')\n",
    "\n",
    "# Load interaction pairs\n",
    "pairs_df = pd.read_csv(PAIRS_FILE)\n",
    "\n",
    "# Process pairs and make predictions\n",
    "predictions = []\n",
    "missing_count = 0\n",
    "\n",
    "print(f\"\\nProcessing {len(pairs_df)} pairs...\")\n",
    "for idx, row in pairs_df.iterrows():\n",
    "    dengue_id = row['dengue_id']\n",
    "    human_id = row['human_id']\n",
    "    \n",
    "    # Get embeddings\n",
    "    human_emb = human_embeddings.get(human_id)\n",
    "    dengue_emb = dengue_embeddings.get(dengue_id)\n",
    "    \n",
    "    if human_emb is None or dengue_emb is None:\n",
    "        missing_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Combine embeddings\n",
    "    combined = np.concatenate([human_emb, dengue_emb])\n",
    "    if combined.shape != (expected_input_dim,):\n",
    "        raise ValueError(f\"Dimension mismatch: Expected {expected_input_dim}, \"\n",
    "                         f\"got {combined.shape[0]}\")\n",
    "    \n",
    "    # Predict\n",
    "    pred_prob = model.predict(combined.reshape(1, -1), verbose=0)[0][0]\n",
    "    predictions.append({\n",
    "        'dengue_id': dengue_id,\n",
    "        'human_id': human_id,\n",
    "        'predicted_label': int(pred_prob >= 0.5),\n",
    "        'confidence': float(pred_prob)\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "if predictions:\n",
    "    results_df = pd.DataFrame(predictions)\n",
    "    results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nSaved {len(predictions)} predictions to {OUTPUT_FILE}\")\n",
    "    print(\"Sample predictions:\")\n",
    "    print(results_df.head())\n",
    "else:\n",
    "    print(\"\\nNo predictions made - check embedding files\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nProcessing Summary:\")\n",
    "print(f\"Total pairs: {len(pairs_df)}\")\n",
    "print(f\"Successful predictions: {len(predictions)}\")\n",
    "print(f\"Missing embeddings: {missing_count}\")\n",
    "print(f\"Prediction rate: {len(predictions)/len(pairs_df)*100:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

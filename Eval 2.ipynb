{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8063f724-2d5d-4f0e-8040-43b02176ca3f",
   "metadata": {},
   "source": [
    "### Task1\n",
    "#### Mapping the unknown_pair_den_human_One.txt names to the actual sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eac42b5-93e5-4f12-ae44-d89fa2f6ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pairs:\n",
      "               dengue_id human_id\n",
      "0  ATQ48923.1_capsid_DV2   TRMT11\n",
      "1  ATQ48923.1_capsid_DV2     GCH1\n",
      "2  ATQ48923.1_capsid_DV2     IGF1\n",
      "3  ATQ48923.1_capsid_DV2     EMC9\n",
      "4  ATQ48923.1_capsid_DV2  DYNLT2B\n",
      "\n",
      "Parsing dengue sequences...\n",
      "Found 60 dengue sequences\n",
      "\n",
      "Parsing human sequences...\n",
      "Found 17806 human sequences\n",
      "\n",
      "Successfully saved 5915 matched pairs\n",
      "Sample output:\n",
      "               dengue_id human_id  \\\n",
      "0  ATQ48923.1_capsid_DV2   TRMT11   \n",
      "1  ATQ48923.1_capsid_DV2     GCH1   \n",
      "2  ATQ48923.1_capsid_DV2     IGF1   \n",
      "3  ATQ48923.1_capsid_DV2     EMC9   \n",
      "4  ATQ48923.1_capsid_DV2  DYNLT2B   \n",
      "\n",
      "                                     dengue_sequence  \\\n",
      "0  FFNMLKRERNRVSTVQQLTKRFSLGMLQGRGPLKLFMALVAFLRFL...   \n",
      "1  FFNMLKRERNRVSTVQQLTKRFSLGMLQGRGPLKLFMALVAFLRFL...   \n",
      "2  FFNMLKRERNRVSTVQQLTKRFSLGMLQGRGPLKLFMALVAFLRFL...   \n",
      "3  FFNMLKRERNRVSTVQQLTKRFSLGMLQGRGPLKLFMALVAFLRFL...   \n",
      "4  FFNMLKRERNRVSTVQQLTKRFSLGMLQGRGPLKLFMALVAFLRFL...   \n",
      "\n",
      "                                      human_sequence  \n",
      "0  MALSCTLNRYLLLMAQEHLEFRLPEIKSLLLLFGGQFASSQETYGK...  \n",
      "1  MEKGPVRAPAEKPRGARCSNGFPERDPPRPGPSRPAEKPPRPEAKS...  \n",
      "2  MGKISSLPTQLFKCCFCDFLKVKMHTMSSSHLFYLALCLLTFTSSA...  \n",
      "3  MGFAHSWLLTPLIFPLHSPGPLALKIAGRIAEFFPDAVLIMLDNQK...  \n",
      "4      MATSIGVSFSVGDGVPEAEKNAGEPENTYILRPVFQQRRVQALCG  \n",
      "\n",
      "Processing Summary:\n",
      "Total input pairs: 6000\n",
      "Matched pairs: 5915\n",
      "Pairs missing dengue: 0\n",
      "Pairs missing human: 85\n",
      "Unmatched rate: 1.4%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_fasta(filename, strip_char=None):\n",
    "    \"\"\"Parse FASTA file into a dictionary, optionally stripping characters from sequences\"\"\"\n",
    "    sequences = {}\n",
    "    current_id = None\n",
    "    current_seq = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if current_id is not None:\n",
    "                    # Save previous sequence\n",
    "                    full_seq = ''.join(current_seq)\n",
    "                    if strip_char:\n",
    "                        full_seq = full_seq.rstrip(strip_char)\n",
    "                    sequences[current_id] = full_seq\n",
    "                # Get new ID (remove '>' and any leading/trailing whitespace)\n",
    "                current_id = line[1:].split()[0].strip()\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "        \n",
    "        # Add the last sequence\n",
    "        if current_id is not None:\n",
    "            full_seq = ''.join(current_seq)\n",
    "            if strip_char:\n",
    "                full_seq = full_seq.rstrip(strip_char)\n",
    "            sequences[current_id] = full_seq\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Load the pairs file\n",
    "try:\n",
    "    pairs_df = pd.read_csv('unknown_pair_den_human_One.txt', sep='\\t', header=None, \n",
    "                          names=['dengue_id', 'human_id'], dtype='string')\n",
    "    print(\"Successfully loaded pairs:\")\n",
    "    print(pairs_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading pairs file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Parse sequence files\n",
    "print(\"\\nParsing dengue sequences...\")\n",
    "dengue_sequences = parse_fasta('dengue_ni_sequences.txt')\n",
    "print(f\"Found {len(dengue_sequences)} dengue sequences\")\n",
    "\n",
    "print(\"\\nParsing human sequences...\")\n",
    "human_sequences = parse_fasta('identical_sequences.txt', strip_char='*')\n",
    "print(f\"Found {len(human_sequences)} human sequences\")\n",
    "\n",
    "# Match pairs with sequences\n",
    "matched_data = []\n",
    "missing_dengue = 0\n",
    "missing_human = 0\n",
    "\n",
    "for idx, row in pairs_df.iterrows():\n",
    "    dengue_id = row['dengue_id']\n",
    "    human_id = row['human_id']\n",
    "    \n",
    "    # Check both sequences exist\n",
    "    has_dengue = dengue_id in dengue_sequences\n",
    "    has_human = human_id in human_sequences\n",
    "    \n",
    "    if not has_dengue:\n",
    "        missing_dengue += 1\n",
    "    if not has_human:\n",
    "        missing_human += 1\n",
    "    \n",
    "    if has_dengue and has_human:\n",
    "        matched_data.append({\n",
    "            'dengue_id': dengue_id,\n",
    "            'human_id': human_id,\n",
    "            'dengue_sequence': dengue_sequences[dengue_id],\n",
    "            'human_sequence': human_sequences[human_id]\n",
    "        })\n",
    "\n",
    "# Create DataFrame and save\n",
    "if matched_data:\n",
    "    output_df = pd.DataFrame(matched_data)\n",
    "    output_df.to_csv('task1_sequences.csv', index=False)\n",
    "    print(f\"\\nSuccessfully saved {len(output_df)} matched pairs\")\n",
    "    print(\"Sample output:\")\n",
    "    print(output_df.head())\n",
    "else:\n",
    "    print(\"\\nNo matching pairs found!\")\n",
    "\n",
    "# Print summary stats\n",
    "print(\"\\nProcessing Summary:\")\n",
    "print(f\"Total input pairs: {len(pairs_df)}\")\n",
    "print(f\"Matched pairs: {len(matched_data)}\")\n",
    "print(f\"Pairs missing dengue: {missing_dengue}\")\n",
    "print(f\"Pairs missing human: {missing_human}\")\n",
    "print(f\"Unmatched rate: {(missing_dengue + missing_human)/len(pairs_df)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77215a3f-a2c1-4d6b-8c20-e33dfcf56109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               dengue_id human_id  \\\n",
      "0  ATQ48923.1_capsid_DV2   TRMT11   \n",
      "1  ATQ48923.1_capsid_DV2     GCH1   \n",
      "2  ATQ48923.1_capsid_DV2     IGF1   \n",
      "3  ATQ48923.1_capsid_DV2     EMC9   \n",
      "4  ATQ48923.1_capsid_DV2  DYNLT2B   \n",
      "\n",
      "                                     dengue_sequence  \\\n",
      "0  FFNMLKRERNRVSTVQQLTKRFSLGMLQGRGPLKLFMALVAFLRFL...   \n",
      "1  FFNMLKRERNRVSTVQQLTKRFSLGMLQGRGPLKLFMALVAFLRFL...   \n",
      "2  FFNMLKRERNRVSTVQQLTKRFSLGMLQGRGPLKLFMALVAFLRFL...   \n",
      "3  FFNMLKRERNRVSTVQQLTKRFSLGMLQGRGPLKLFMALVAFLRFL...   \n",
      "4  FFNMLKRERNRVSTVQQLTKRFSLGMLQGRGPLKLFMALVAFLRFL...   \n",
      "\n",
      "                                      human_sequence  \n",
      "0  MALSCTLNRYLLLMAQEHLEFRLPEIKSLLLLFGGQFASSQETYGK...  \n",
      "1  MEKGPVRAPAEKPRGARCSNGFPERDPPRPGPSRPAEKPPRPEAKS...  \n",
      "2  MGKISSLPTQLFKCCFCDFLKVKMHTMSSSHLFYLALCLLTFTSSA...  \n",
      "3  MGFAHSWLLTPLIFPLHSPGPLALKIAGRIAEFFPDAVLIMLDNQK...  \n",
      "4      MATSIGVSFSVGDGVPEAEKNAGEPENTYILRPVFQQRRVQALCG  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('task1_sequences.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e676c8-4dfc-4633-ab3d-9752b01b0b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ea7a296-32b7-4152-b4f9-070f43f5631f",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "#### Now we will take the file from task1 and get the top 100 kmers and map them to the corresponding kmer embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83412171-88dc-4d45-bb58-2f0952458881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading k-mer embeddings...\n",
      "Loaded 157689 k-mers with 128D embeddings\n",
      "Loading models...\n",
      "\n",
      "Model Architecture Verification:\n",
      "Human model input shape: (None, 100, 128)\n",
      "Human model output shape: (None, 100, 128)\n",
      "Dengue model input shape: (None, 100, 128)\n",
      "Dengue model output shape: (None, 100, 128)\n",
      "\n",
      "Processing human sequences...\n",
      "Saved 5915 human embeddings\n",
      "\n",
      "Processing dengue sequences...\n",
      "Saved 5915 dengue embeddings\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "KMER_LENGTH = 4\n",
    "TOP_KMERS = 100\n",
    "EMBEDDING_DIM = 128  # Dimension from k-mer embedding file\n",
    "LATENT_DIM = 128     # Final embedding dimension\n",
    "\n",
    "def load_kmer_embeddings(filename):\n",
    "    \"\"\"Load k-mer embeddings with validation\"\"\"\n",
    "    kmer_emb = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != EMBEDDING_DIM + 1:  # 1 k-mer + 128 values\n",
    "                continue\n",
    "            kmer = parts[0]\n",
    "            emb = list(map(float, parts[1:]))\n",
    "            kmer_emb[kmer] = emb\n",
    "    print(f\"Loaded {len(kmer_emb)} k-mers with {EMBEDDING_DIM}D embeddings\")\n",
    "    return kmer_emb\n",
    "\n",
    "def generate_kmers(sequence, k):\n",
    "    \"\"\"Generate k-mers with length validation\"\"\"\n",
    "    return [sequence[i:i+k] for i in range(len(sequence)-k+1) if len(sequence[i:i+k]) == k]\n",
    "\n",
    "def get_top_kmer_features(sequence, kmer_emb, k=4, top_n=100):\n",
    "    \"\"\"Get top k-mers with proper frequency handling\"\"\"\n",
    "    kmers = generate_kmers(sequence, k)\n",
    "    freq = defaultdict(int)\n",
    "    for kmer in kmers:\n",
    "        freq[kmer] += 1\n",
    "    \n",
    "    # Sort by frequency then alphabetically\n",
    "    sorted_kmers = sorted(freq.items(), \n",
    "                         key=lambda x: (-x[1], x[0]))[:top_n*2]\n",
    "    \n",
    "    # Select unique k-mers with embeddings\n",
    "    selected = []\n",
    "    for kmer, _ in sorted_kmers:\n",
    "        if kmer in kmer_emb and kmer not in selected:\n",
    "            selected.append(kmer)\n",
    "            if len(selected) == top_n:\n",
    "                break\n",
    "    \n",
    "    # Fill missing with zero vectors\n",
    "    embeddings = []\n",
    "    for kmer in selected[:top_n]:\n",
    "        embeddings.append(kmer_emb.get(kmer, [0.0]*EMBEDDING_DIM))\n",
    "    \n",
    "    # Pad if needed\n",
    "    while len(embeddings) < top_n:\n",
    "        embeddings.append([0.0]*EMBEDDING_DIM)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "def process_sequences(df, model, kmer_emb, id_col, sequence_col):\n",
    "    \"\"\"Process sequences and generate embeddings\"\"\"\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        seq_id = row[id_col]\n",
    "        sequence = row[sequence_col].replace('*', '')\n",
    "        \n",
    "        # Get k-mer features\n",
    "        kmer_features = get_top_kmer_features(sequence, kmer_emb)\n",
    "        \n",
    "        # Reshape for model input: (batch_size, timesteps, features)\n",
    "        input_data = kmer_features.reshape(1, TOP_KMERS, EMBEDDING_DIM)\n",
    "        \n",
    "        # Get latent embedding\n",
    "        embedding = model.predict(input_data, verbose=0)\n",
    "        \n",
    "        # Extract final 128D embedding (last timestep)\n",
    "        latent_embedding = embedding[0, -1, :].tolist()\n",
    "        \n",
    "        # Create row with separate columns for each embedding dimension\n",
    "        emb_dict = {f\"emb_{i+1}\": val for i, val in enumerate(latent_embedding)}\n",
    "        results.append({id_col: seq_id, **emb_dict})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load resources\n",
    "print(\"Loading k-mer embeddings...\")\n",
    "kmer_embeddings = load_kmer_embeddings('kmers_embedding_human_dengue.txt')\n",
    "\n",
    "print(\"Loading models...\")\n",
    "human_ae = tf.keras.models.load_model('drop_Human_bilstm_autoencoder_model.keras')\n",
    "dengue_ae = tf.keras.models.load_model('drop_Dengue_bilstm_autoencoder_model.keras')\n",
    "\n",
    "# Verify model output shapes\n",
    "print(\"\\nModel Architecture Verification:\")\n",
    "print(\"Human model input shape:\", human_ae.input_shape)\n",
    "print(\"Human model output shape:\", human_ae.output_shape)\n",
    "print(\"Dengue model input shape:\", dengue_ae.input_shape)\n",
    "print(\"Dengue model output shape:\", dengue_ae.output_shape)\n",
    "\n",
    "# Load sequences\n",
    "task1_df = pd.read_csv('task1_sequences.csv')\n",
    "\n",
    "# Process human sequences\n",
    "print(\"\\nProcessing human sequences...\")\n",
    "human_emb_df = process_sequences(task1_df, human_ae, kmer_embeddings, \n",
    "                                'human_id', 'human_sequence')\n",
    "human_emb_df.to_csv('human_embeddings_128d.csv', index=False)\n",
    "print(f\"Saved {len(human_emb_df)} human embeddings\")\n",
    "\n",
    "# Process dengue sequences\n",
    "print(\"\\nProcessing dengue sequences...\")\n",
    "dengue_emb_df = process_sequences(task1_df, dengue_ae, kmer_embeddings,\n",
    "                                 'dengue_id', 'dengue_sequence')\n",
    "dengue_emb_df.to_csv('dengue_embeddings_128d.csv', index=False)\n",
    "print(f\"Saved {len(dengue_emb_df)} dengue embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312049f6-f173-420d-8b1d-323caaac6b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  human_id     emb_1     emb_2     emb_3     emb_4    emb_5     emb_6   emb_7  \\\n",
      "0   TRMT11  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.0000   \n",
      "1     GCH1  0.745591  0.000000  0.199013  0.000000  0.00000  0.000000  0.0000   \n",
      "2     IGF1  0.000000  0.013195  0.000000  0.669323  0.13289  0.052717  0.0853   \n",
      "3     EMC9  0.000000  0.000000  0.506422  0.000000  0.00000  0.132925  0.0000   \n",
      "4  DYNLT2B  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.0000   \n",
      "\n",
      "      emb_8     emb_9  ...   emb_119   emb_120   emb_121   emb_122   emb_123  \\\n",
      "0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1  0.212964  0.000000  ...  0.223052  0.090198  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.034791  ...  0.033533  0.011158  0.000000  0.000000  0.078731   \n",
      "3  0.000000  0.025503  ...  0.000000  0.013507  0.381108  0.266483  0.000000   \n",
      "4  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "    emb_124   emb_125  emb_126   emb_127   emb_128  \n",
      "0  0.000000  0.000000      0.0  0.000000  0.000000  \n",
      "1  0.134721  0.053629      0.0  0.000000  0.318182  \n",
      "2  0.000000  0.000000      0.0  0.020485  0.376919  \n",
      "3  0.000000  0.000000      0.0  0.000000  0.632892  \n",
      "4  0.000000  0.000000      0.0  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 129 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('human_embeddings_128d.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a492f8-6e0a-4711-8117-14d6a3065e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               dengue_id  emb_1  emb_2  emb_3  emb_4  emb_5  emb_6  emb_7  \\\n",
      "0  ATQ48923.1_capsid_DV2    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1  ATQ48923.1_capsid_DV2    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2  ATQ48923.1_capsid_DV2    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3  ATQ48923.1_capsid_DV2    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4  ATQ48923.1_capsid_DV2    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "   emb_8     emb_9  ...   emb_119  emb_120  emb_121   emb_122  emb_123  \\\n",
      "0    0.0  0.172186  ...  0.047736      0.0      0.0  0.141219      0.0   \n",
      "1    0.0  0.172186  ...  0.047736      0.0      0.0  0.141219      0.0   \n",
      "2    0.0  0.172186  ...  0.047736      0.0      0.0  0.141219      0.0   \n",
      "3    0.0  0.172186  ...  0.047736      0.0      0.0  0.141219      0.0   \n",
      "4    0.0  0.172186  ...  0.047736      0.0      0.0  0.141219      0.0   \n",
      "\n",
      "    emb_124  emb_125  emb_126  emb_127  emb_128  \n",
      "0  0.117554      0.0      0.0      0.0      0.0  \n",
      "1  0.117554      0.0      0.0      0.0      0.0  \n",
      "2  0.117554      0.0      0.0      0.0      0.0  \n",
      "3  0.117554      0.0      0.0      0.0      0.0  \n",
      "4  0.117554      0.0      0.0      0.0      0.0  \n",
      "\n",
      "[5 rows x 129 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dengue_embeddings_128d.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e0f5b-5211-4ef3-b15e-7318fa6b0b0e",
   "metadata": {},
   "source": [
    "### Task3\n",
    "#### Get the label for the given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e378bda-e683-47b1-83ad-38837f85211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prediction model...\n",
      "Model expects input dimension: 256\n",
      "\n",
      "Loading human embeddings...\n",
      "Loaded 3052 embeddings from human_embeddings_128d.csv\n",
      "Loading dengue embeddings...\n",
      "Loaded 60 embeddings from dengue_embeddings_128d.csv\n",
      "\n",
      "Processing 5915 pairs...\n",
      "\n",
      "Saved 5915 predictions to eval2_final_predictions.csv\n",
      "Sample predictions:\n",
      "               dengue_id human_id  predicted_label    confidence\n",
      "0  ATQ48923.1_capsid_DV2   TRMT11                1  1.000000e+00\n",
      "1  ATQ48923.1_capsid_DV2     GCH1                0  1.894588e-20\n",
      "2  ATQ48923.1_capsid_DV2     IGF1                0  9.157673e-11\n",
      "3  ATQ48923.1_capsid_DV2     EMC9                0  5.384458e-24\n",
      "4  ATQ48923.1_capsid_DV2  DYNLT2B                1  1.000000e+00\n",
      "\n",
      "Processing Summary:\n",
      "Total pairs: 5915\n",
      "Successful predictions: 5915\n",
      "Missing embeddings: 0\n",
      "Prediction rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configuration\n",
    "HUMAN_EMBED_FILE = 'human_embeddings_128d.csv'\n",
    "DENGUE_EMBED_FILE = 'dengue_embeddings_128d.csv'\n",
    "PAIRS_FILE = 'task1_sequences.csv'\n",
    "MODEL_PATH = 'fc_final_model.keras'\n",
    "OUTPUT_FILE = 'eval2_final_predictions.csv'\n",
    "\n",
    "def load_embeddings(file_path, id_col='human_id'):\n",
    "    \"\"\"Load embeddings from wide-format CSV with emb_1 to emb_128 columns\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract embedding columns\n",
    "    emb_cols = [col for col in df.columns if col.startswith('emb_')]\n",
    "    \n",
    "    # Validate embedding dimensions\n",
    "    if len(emb_cols) != 128:\n",
    "        raise ValueError(f\"Invalid embedding dimensions in {file_path}: \"\n",
    "                         f\"Expected 128 columns, found {len(emb_cols)}\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    embeddings = {}\n",
    "    for _, row in df.iterrows():\n",
    "        emb_array = np.array([row[col] for col in emb_cols], dtype=np.float32)\n",
    "        if emb_array.shape != (128,):\n",
    "            raise ValueError(f\"Invalid embedding shape for {row[id_col]}: \"\n",
    "                             f\"Expected (128,), got {emb_array.shape}\")\n",
    "        embeddings[row[id_col]] = emb_array\n",
    "    \n",
    "    print(f\"Loaded {len(embeddings)} embeddings from {file_path}\")\n",
    "    return embeddings\n",
    "\n",
    "# Load pretrained model\n",
    "print(\"Loading prediction model...\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "expected_input_dim = model.input_shape[1]\n",
    "print(f\"Model expects input dimension: {expected_input_dim}\")\n",
    "\n",
    "# Load embeddings\n",
    "print(\"\\nLoading human embeddings...\")\n",
    "human_embeddings = load_embeddings(HUMAN_EMBED_FILE, 'human_id')\n",
    "print(\"Loading dengue embeddings...\")\n",
    "dengue_embeddings = load_embeddings(DENGUE_EMBED_FILE, 'dengue_id')\n",
    "\n",
    "# Load interaction pairs\n",
    "pairs_df = pd.read_csv(PAIRS_FILE)\n",
    "\n",
    "# Process pairs and make predictions\n",
    "predictions = []\n",
    "missing_count = 0\n",
    "\n",
    "print(f\"\\nProcessing {len(pairs_df)} pairs...\")\n",
    "for idx, row in pairs_df.iterrows():\n",
    "    dengue_id = row['dengue_id']\n",
    "    human_id = row['human_id']\n",
    "    \n",
    "    # Get embeddings\n",
    "    human_emb = human_embeddings.get(human_id)\n",
    "    dengue_emb = dengue_embeddings.get(dengue_id)\n",
    "    \n",
    "    if human_emb is None or dengue_emb is None:\n",
    "        missing_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Combine embeddings\n",
    "    combined = np.concatenate([human_emb, dengue_emb])\n",
    "    if combined.shape != (expected_input_dim,):\n",
    "        raise ValueError(f\"Dimension mismatch: Expected {expected_input_dim}, \"\n",
    "                         f\"got {combined.shape[0]}\")\n",
    "    \n",
    "    # Predict\n",
    "    pred_prob = model.predict(combined.reshape(1, -1), verbose=0)[0][0]\n",
    "    predictions.append({\n",
    "        'dengue_id': dengue_id,\n",
    "        'human_id': human_id,\n",
    "        'predicted_label': int(pred_prob >= 0.5),\n",
    "        'confidence': float(pred_prob)\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "if predictions:\n",
    "    results_df = pd.DataFrame(predictions)\n",
    "    results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nSaved {len(predictions)} predictions to {OUTPUT_FILE}\")\n",
    "    print(\"Sample predictions:\")\n",
    "    print(results_df.head())\n",
    "else:\n",
    "    print(\"\\nNo predictions made - check embedding files\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nProcessing Summary:\")\n",
    "print(f\"Total pairs: {len(pairs_df)}\")\n",
    "print(f\"Successful predictions: {len(predictions)}\")\n",
    "print(f\"Missing embeddings: {missing_count}\")\n",
    "print(f\"Prediction rate: {len(predictions)/len(pairs_df)*100:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
